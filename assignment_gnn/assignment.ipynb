{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c9cdf41",
   "metadata": {},
   "source": [
    "# Assignment — Graph neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce81eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange\n",
    "import numpy as np\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34817910",
   "metadata": {},
   "source": [
    "### CORA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8217ac0c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pQGhI8qBbVIM",
    "outputId": "29e2a7b9-51d8-4b46-b0bd-d9b28b647162"
   },
   "outputs": [],
   "source": [
    "!pip install dgl dglgo -f https://data.dgl.ai/wheels/repo.html -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f7e59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl.data import CoraGraphDataset\n",
    "from dgl import function as fn\n",
    "from dgl.nn import SAGEConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a52628",
   "metadata": {
    "id": "mf6KqavDvSoU"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f992762",
   "metadata": {},
   "source": [
    "The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 10556/2 links. Let us take a closer look at this dataset. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3876dd0c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kTetgXDXzjnu",
    "outputId": "05465fea-279c-43f9-9c3a-a03007db031f"
   },
   "outputs": [],
   "source": [
    "data = CoraGraphDataset(force_reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60459d90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tBJ_L77O0h0l",
    "outputId": "1f61b270-2b4a-44de-c0ac-a003bb783f66"
   },
   "outputs": [],
   "source": [
    "graph = data[0]\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e97b108",
   "metadata": {},
   "source": [
    "Adjacency matrix can be obtained as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc92084",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XlByC4PmweNb",
    "outputId": "b12fc24a-32da-4fb5-8fc3-b1e31d2eeef7"
   },
   "outputs": [],
   "source": [
    "adj = graph.adj().to_dense()\n",
    "adj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613d60e2",
   "metadata": {},
   "source": [
    "Feature matrix is stored in node features `feat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe904f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N6tSuSip0_4x",
    "outputId": "8b4d3c3e-9255-433e-b131-44414f988464"
   },
   "outputs": [],
   "source": [
    "feat = graph.ndata['feat']\n",
    "print(feat.shape)\n",
    "feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22e2f36",
   "metadata": {},
   "source": [
    "In DGL, Cora feature matrix are normalized so that the sum per row is 1. Labels are represent type of the publication, they are stored in `label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c85b87c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mEjkhbW-1B7B",
    "outputId": "099191ec-be12-4c2f-8eef-1d03b97f5793"
   },
   "outputs": [],
   "source": [
    "label = graph.ndata['label']\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffdf1c0",
   "metadata": {},
   "source": [
    "Here are train, validation and test masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cb7ad4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vMIQwuZz1HLT",
    "outputId": "29137f18-15b3-4733-9798-caf037f9664f"
   },
   "outputs": [],
   "source": [
    "train_mask = graph.ndata['train_mask']\n",
    "val_mask = graph.ndata['val_mask']\n",
    "test_mask = graph.ndata['test_mask']\n",
    "val_mask, train_mask, test_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe37030e",
   "metadata": {},
   "source": [
    "### Task 1. Graph convolution as matrix product (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc79913",
   "metadata": {},
   "source": [
    "Graph convolutional network (GCN) was proposed in [[1]](#references) and defined on an undirected graph as the following propagation rule:\n",
    "\n",
    "$$H^{(l)}=\\sigma\\left(\\tilde D^{-1 / 2} \\tilde A \\tilde D^{-1 / 2} H^{(l-1)} W^{(l)}\\right)$$\n",
    "\n",
    "where $\\tilde A = A + I_N$ is the adjacency matrix with added self-connections. $I_N$ is the identity matrix, $\\tilde D_{ii} = \\sum_j \\tilde A_{ij}$ is the degree matrix and $W^{(l)}$ is trainable matrix at the layer $l$. $\\sigma$ represents non-linear activation function such as ReLU. $H^{(l)}$ is the hidden states at the layer $l$ and $H^{(0)} = X$ is the initial feature matrix.\n",
    "\n",
    "Write a class `GCNMatrixProductLayer` that takes adjacency matrix, hidden states and returns the next hidden states before activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff5b791",
   "metadata": {
    "deletable": false,
    "id": "f_mBC5dKnnww",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8e8bb1c6da30605ca8ba49a6c4ecfa9",
     "grade": false,
     "grade_id": "cell-b71f48a8f9c231a0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GCNMatrixProductLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(in_dim, out_dim)\n",
    "        \n",
    "    def forward(self, adj, h):\n",
    "        _adj = adj.clone()\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904fa061",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "8lynaeRSot46",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "508f28b0a571960c10b36bb656874b59",
     "grade": true,
     "grade_id": "cell-59516dd351a3b233",
     "locked": true,
     "points": 0.0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "ec164c99-e61b-4f76-d52f-f9869a46570a"
   },
   "outputs": [],
   "source": [
    "layer = GCNMatrixProductLayer(in_dim=2708, out_dim=10)\n",
    "with torch.no_grad():\n",
    "    layer.dense.weight.fill_(0.1)\n",
    "    layer.dense.bias.fill_(0.1)\n",
    "    h = layer(adj, torch.eye(2708))\n",
    "assert round(h[0, 0].item(), 2) == 0.19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b041722",
   "metadata": {},
   "source": [
    "Write a class `GCNMatrixProduct` that takes the adjacency matrix and feature matrix and performs two-layer convolution with intermediate ReLU activation. The class initialized by input, hidden and output dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9ef4e4",
   "metadata": {
    "deletable": false,
    "id": "KqTq6pF05jM-",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "856ad1e1e46a477d61fb9ebbde928fa4",
     "grade": false,
     "grade_id": "cell-5cecc368828015e4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GCNMatrixProduct(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, out_dim):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    def forward(self, adj, feat):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b3e790",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20891f674b1f17ad9b39e5e7323599cd",
     "grade": true,
     "grade_id": "cell-13a9aa3e489245ee",
     "locked": true,
     "points": 0.0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = GCNMatrixProduct(2708, 32, 7)\n",
    "logits = model(adj, torch.eye(2708))\n",
    "assert logits.shape == (2708, 7)\n",
    "assert logits.min() < 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217ab100",
   "metadata": {},
   "source": [
    "Let us create the two-layer GCN for node classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83c6c89",
   "metadata": {
    "id": "3ugUQ8Ts50bm"
   },
   "outputs": [],
   "source": [
    "model = GCNMatrixProduct(1433, 32, 7)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "log = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0569ff05",
   "metadata": {},
   "source": [
    "We train the model in semi-supervised setting: propagate information over the full graph, but optimize it by only train cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e7d012",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313,
     "referenced_widgets": [
      "4fa7c5a860a74ae0be0404b2842ec809",
      "2307d5ed730f4c6fa82e8a178c243d0d",
      "b1270003ede94d53bfdd2df025c772ee",
      "10ec7d4c95d4495cb71f2c04ea634ee0",
      "cbd72762d02c41f2a34916daf6114d22",
      "8106dd0f5e1f4b0c9c84954bdce1d579",
      "42708e15359446719803bb38facedbab",
      "796b2558fb2f43e396b0f68b01b3a6fb",
      "08ae5472e6524b86a7d9844f087f1b83",
      "76c2090b6819432ab829350fa469d75e",
      "36ba71ca6f1d429b96e01b3e3335c970"
     ]
    },
    "id": "A5734iOi6O5x",
    "outputId": "63c71b97-2ab3-4081-b817-2cc866d574d5"
   },
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "for i in trange(n_epochs):\n",
    "    \n",
    "    logits = model(adj, feat)\n",
    "    train_loss = F.cross_entropy(logits[train_mask], label[train_mask])\n",
    "\n",
    "    opt.zero_grad()\n",
    "    train_loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loss = F.cross_entropy(logits[val_mask], label[val_mask])\n",
    "    \n",
    "    log.append([train_loss.item(), val_loss.item()])\n",
    "    \n",
    "plt.plot(np.array(log))\n",
    "plt.title('Loss')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e222fe4",
   "metadata": {},
   "source": [
    "We evaluate the model by balanced accuracy that accounts for inbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7945090a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "_fV6MI9q6_GL",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b39b99f78ca3f8319ac2b523edc550a9",
     "grade": true,
     "grade_id": "cell-d77d156cd37bfe99",
     "locked": true,
     "points": 0.0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "52c02a31-1298-43b4-8065-6952aef793c1"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(adj, feat)\n",
    "y_pred = torch.argmax(logits[test_mask], 1)\n",
    "score = balanced_accuracy_score(label[test_mask], y_pred)\n",
    "assert score > 0.7\n",
    "print(f'Balanced accuracy: {score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee21f3a",
   "metadata": {},
   "source": [
    "### Task 2. Graph convolution as message passing (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23ffa87",
   "metadata": {},
   "source": [
    "The original GCN is computationally expensive for large graphs since it requires to compute matrix product $\\tilde D^{-1 / 2} \\tilde A \\tilde D^{-1 / 2} H^{l}$, that costs $O(|V|^2)$. However, the information is propagated along edges, the number of which is linearly depends on the number of nodes in scale-free networks, that is $O(|V|)$. Message passing framework was proposed in [[1]](#references) to reduce computation costs by representing a graph neural network's layer as the composition of two functions:\n",
    "\n",
    "$$m_{ij}^l = \\text{Message}\\left(h_i^{l-1}, h_j^{l-1}\\right)$$\n",
    "$$h_i^l = \\text{Reduce}_{j \\in \\mathcal{N}_\\text{in}(i)}\\left(m_{ji}^l\\right)$$\n",
    "\n",
    "where $\\text{Message}(\\cdot)$ is the message function that applied on each edges in the graph and stores computed values (messages) on edges. $\\text{Reduce}_{j \\in \\mathcal{N}_\\text{in}(i)}(\\cdot)$ is the reduce (aka aggregation, reduction) function that applied on each node and aggregates messages from incoming links (mailboxes). \n",
    "\n",
    "<img src='https://raw.githubusercontent.com/netspractice/network-science/main/images/messpass.png' width=600>\n",
    "\n",
    "In particular, GCN can be represented as follows:\n",
    "\n",
    "$$h_i^l = \\sigma \\left(W^l \\sum_{j \\in \\mathcal{N}_\\text{in}(i)}\\frac{h_j^{l-1}}{\\sqrt{|\\mathcal{N}_\\text{in}(j)||\\mathcal{N}_\\text{in}(i)|}}\\right)$$\n",
    "\n",
    "where the message function\n",
    "\n",
    "$$m_{ji}^l = \\frac{W^l h_j^{l-1}}{\\sqrt{|\\mathcal{N}_\\text{in}(j)|}}$$ \n",
    "\n",
    "is the transformed source node's hidden state normalized by the in-degree of the source node. The reduce function\n",
    "\n",
    "$$h_i^l = \\sigma \\left(\\frac{\\sum_{j \\in \\mathcal{N}_\\text{in}(i)} m_{ji}^l}{\\sqrt{|\\mathcal{N}_\\text{in}(i)|}}\\right)$$\n",
    "\n",
    "is the sum over all incoming neighbors' hidden states with in-degree normalization and subsequent activation $\\sigma$. Here we assume that the graph includes self-connections, that is a node is in its neighborhood. We also assume that the graph is equivalent to its undirected representation, that is $A = A^\\top$.\n",
    "\n",
    "In DGL, message passing is applied by `graph.update_all(message_func, reduce_func)` where `message_func` is defined on edges and `reduce_func` on nodes. DGL contains built-in message and reduce functions, but it is also possible to define your own functions. GCN can be implemented using following functions: \n",
    "* message function `fn.copy_src(src='h', out='m')` copies the source node feature `h` into the mailbox `m`\n",
    "* reduce function `fn.sum(msg='m', out='h')` sums messages from mailboxes `m` and save the result into the node feature `h`\n",
    "\n",
    "Write a class `GCNMessagePassingLayer` that takes a graph, hidden states and returns the next hidden states before activation.\n",
    "\n",
    "*Hints:*\n",
    "* _adding node features is similar to adding values to a dictionary: `graph.ndata['h'] = h`_\n",
    "* _in-degrees can be computed by `graph.in_degrees()`_\n",
    "\n",
    "*Remark: `graph.local_scope()` makes all operations isolated from the original graphs.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5984036",
   "metadata": {
    "deletable": false,
    "id": "gpikkab-1g61",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7285e6772f70805893fd9df8c98dd43",
     "grade": false,
     "grade_id": "cell-7e663cf5c1afa659",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GCNMessagePassingLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(in_dim, out_dim)\n",
    "    def forward(self, graph, h):\n",
    "        with graph.local_scope():\n",
    "            graph = graph.add_self_loop()\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8f0c5f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "OefKyjCBP5h4",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8073fd1f805718186c8fd3fd29de8351",
     "grade": true,
     "grade_id": "cell-b7b8314f51fe4d0f",
     "locked": true,
     "points": 0.0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "ab615a6c-80bc-4cf5-b12b-ea539bde7f61"
   },
   "outputs": [],
   "source": [
    "layer = GCNMessagePassingLayer(in_dim=2708, out_dim=10)\n",
    "with torch.no_grad():\n",
    "    layer.dense.weight.fill_(0.1)\n",
    "    layer.dense.bias.fill_(0.1)\n",
    "    h = layer(graph, torch.eye(2708))\n",
    "assert round(h[0, 0].item(), 2) == 0.19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa85c8d",
   "metadata": {},
   "source": [
    "Write a class `GCNMessagePassing` that takes the graph and feature matrix and performs two-layer convolution with intermediate ReLU activation. The class initialized by input, hidden and output dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5391849",
   "metadata": {
    "deletable": false,
    "id": "Je2rbOMiAW37",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "648906ac3877307e03e5e86946e862da",
     "grade": false,
     "grade_id": "cell-0e8fb3b6e5858162",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GCNMessagePassing(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, out_dim):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    def forward(self, graph, feat):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c274e7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "39b9ccecacba09fdee61474dd14c580d",
     "grade": true,
     "grade_id": "cell-bbdfaaf84f5f19cf",
     "locked": true,
     "points": 0.0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = GCNMessagePassing(2708, 32, 7)\n",
    "logits = model(graph, torch.eye(2708))\n",
    "assert logits.shape == (2708, 7)\n",
    "assert logits.min() < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8332d3",
   "metadata": {
    "id": "Serljs2Ssy_F"
   },
   "outputs": [],
   "source": [
    "model = GCNMessagePassing(in_dim=1433, hid_dim=32, out_dim=7)\n",
    "opt = Adam(model.parameters(), lr=0.01)\n",
    "log = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8712d82d",
   "metadata": {},
   "source": [
    "We train the model in semi-supervised setting: propagate information over the full graph, but optimize it by only train cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628bc120",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313,
     "referenced_widgets": [
      "79cb6320d5ad480d8cb7af3e7eca1eaa",
      "b796ab1ee5ef404fa3f42eefb29988b2",
      "c70eda97e5fd4378b91dc2f84f17bd7d",
      "a905b95c060842d998fec0b53f107399",
      "6fa79bdfa40e4737a6d834edff1c6e98",
      "7f7e1ce3028d4078abc064f038200453",
      "5acd380e813446a4aabc84d890c1ea9f",
      "963ebc71728c480daa1b3e6d6b017259",
      "3118ff3e3ee640d887cdec0e45079fa3",
      "5fdc14fd269f4af9b96fc1516f792a69",
      "b9a0e4efbae148f58c6603f950cf3df6"
     ]
    },
    "id": "pOO2N3das06E",
    "outputId": "38c25ace-d774-4e3e-d387-c876661817ee"
   },
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "for i in trange(n_epochs):\n",
    "    \n",
    "    logits = model(graph, feat)\n",
    "    train_loss = F.cross_entropy(logits[train_mask], label[train_mask])\n",
    "\n",
    "    opt.zero_grad()\n",
    "    train_loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loss = F.cross_entropy(logits[val_mask], label[val_mask])\n",
    "    \n",
    "    log.append([train_loss.item(), val_loss.item()])\n",
    "    \n",
    "plt.plot(np.array(log))\n",
    "plt.title('Loss')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709249ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "KY_dG01As28Q",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d96adaccfb3d3494968ec7a788b7db10",
     "grade": true,
     "grade_id": "cell-5ec89b78a948cb61",
     "locked": true,
     "points": 0.0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "f3d27ae9-0577-4805-df01-c2e45fe14c2a"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(graph, feat)\n",
    "y_pred = torch.argmax(logits[test_mask], 1)\n",
    "score = balanced_accuracy_score(label[test_mask], y_pred)\n",
    "assert score > 0.7\n",
    "print(f'Balanced accuracy: {score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f7307d",
   "metadata": {},
   "source": [
    "### Task 3. Graph auto-encoder (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53670340",
   "metadata": {},
   "source": [
    "Graph auto-encoder (GAE) was proposed in [[2]](#references) as a simple node embedding model that incorporates node features. The model is consists of two parts:\n",
    "* GCN node encoder $Z = \\text{GCN}(X, A)$ where $X$ is the feature matrix\n",
    "* Dot product decoder $\\hat A = \\sigma(ZZ^\\top)$ where $\\sigma$ is the sigmoid function\n",
    "\n",
    "Write a class `DotProductDecoder` that takes node embeddgins and returns dot product before activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c460db",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94fc1408b8ccdeccd59e8c5d6f6aa5c4",
     "grade": false,
     "grade_id": "cell-a67dd0513e743f58",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class DotProductDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, z):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c730e093",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "627e49028327bee50ff18660bc952fb5",
     "grade": true,
     "grade_id": "cell-4f9b59eac548798b",
     "locked": true,
     "points": 1.0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "decoder = DotProductDecoder()\n",
    "assert torch.all(decoder(torch.eye(2)[:, :1]) * 2 == torch.tensor([[2, 0], [0, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fb8d87",
   "metadata": {},
   "source": [
    "The GAE is trained to reconstruct the adjacency matrix, that is the decoder can be considered as the link prediction model. The reconstruction loss is the binary cross entropy between the adjacency matrix $A$ and reconstructed adjacency matrix $\\hat A$. To overcome the imbalance between positive and negative edges, the loss is computed with positive weights — the number of negative exaples divided by the number of positive examples, that is $(|V|^2 - E)/E$.\n",
    "\n",
    "Write a class `ReconstructionLoss` that takes reconstructed adjacency matrix and computes the binary cross entropy with positive weights `F.binary_cross_entropy_with_logits(label, target, pos_weights=...)`. The class is initialized by the adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38651b0b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e94ea2e39a78930a13843e844add7ee2",
     "grade": false,
     "grade_id": "cell-725f9456c5768b1e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class ReconstructionLoss:\n",
    "    def __init__(self, adj):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def __call__(self, rec_adj):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575a9887",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6822debfb10e4c8eaffb38f83d7011bd",
     "grade": true,
     "grade_id": "cell-e6b239111250755e",
     "locked": true,
     "points": 1.0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "rec_loss = ReconstructionLoss(torch.eye(2))\n",
    "assert round(rec_loss(1 - torch.eye(2)).item(), 4) == 1.0032"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9403b0b",
   "metadata": {},
   "source": [
    "Here we initialize encoder, decoder, reconstruction loss and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb7900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = GCNMessagePassing(in_dim=1433, hid_dim=64, out_dim=32)\n",
    "decoder = DotProductDecoder()\n",
    "rec_loss = ReconstructionLoss(adj)\n",
    "opt = Adam(encoder.parameters(), lr=0.02)\n",
    "log = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5b5f88",
   "metadata": {},
   "source": [
    "Complete the training loop for GAE. Train the model in unsupervised setting: calculate the reconstruction loss over all edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26457fa7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e25fc920277cbb62e8b52c82bd0732c0",
     "grade": false,
     "grade_id": "cell-f1fe4005cbb20b73",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 400\n",
    "for i in trange(n_epochs):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "plt.plot(np.array(log))\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63152cb4",
   "metadata": {},
   "source": [
    "We evaluate GAE by obtaining node embeddings via GCN encoder and training the logistic regression for node classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8219af43",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef3b36a212c279031e8dd78a9afeae0e",
     "grade": true,
     "grade_id": "cell-85a3ec5453dd8fc2",
     "locked": true,
     "points": 1.0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    X = encoder.forward(graph, feat).numpy()\n",
    "y = label.numpy()\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X[train_mask], y[train_mask])\n",
    "\n",
    "y_true = y[test_mask]\n",
    "y_pred = lr.predict(X[test_mask])\n",
    "score = balanced_accuracy_score(y_true, y_pred)\n",
    "assert score >= 0.6\n",
    "print(f'Balanced accuracy: {score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9a163e",
   "metadata": {},
   "source": [
    "Let us visualize node embeddings via t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd996e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_emb = TSNE(n_components=2).fit_transform(X)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title('Node embeddings of graph auto-encoder')\n",
    "scatter = plt.scatter(xy_emb[:, 0], xy_emb[:, 1], c=label, s=10, cmap=plt.cm.Set2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46be1142",
   "metadata": {},
   "source": [
    "Note that GAE is the simplest encoder-decoder graph model that can can be extended to the variational graph auto-encoder (VGAE) [[2]](#references) and GraphVAE [[1]](#references)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aace5544",
   "metadata": {},
   "source": [
    "### Task 4. Graph attention network (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda10446",
   "metadata": {},
   "source": [
    "The graph attention network (GAT) was proposed in [[4]](#references). The idea behind GAT is based on the multi-head self-attention mechanism [[6]](#references). The graph attention layer is defined by message passing as follows:\n",
    "\n",
    "$$h_i^{l} = \\sum_{j \\in \\mathcal{N}(i)}\\alpha_{ij}^{l}W_\\text{node}^{l}h_j^{l-1}$$\n",
    "\n",
    "where $\\alpha_{ij}$ is the attention score defined on each edge:\n",
    "\n",
    "$$\\alpha_{ij}^{l} = \\text{Softmax}_i(e_{ij}^{l})$$\n",
    "\n",
    "where $e_{ij}^{l}$ is the edge hidden state obtained by the linear transformation of concatenatenated nodes' hidden states and subsequent LeakyReLU activation:\n",
    "\n",
    "$$e_{ij}^{l} = \\text{LeakyReLU}(W_\\text{edge}^l[W_\\text{node}^{l}h_i^{l-1}||W_\\text{node}^{l}h_j^{l-1}])$$\n",
    "\n",
    "where $[\\cdot||\\cdot]$ is concatenation. The multi-head self-attention is defined by different types of attention score (heads) $(\\alpha_{ij}^{l})_0, (\\alpha_{ij}^{l})_1, ..., (\\alpha_{ij}^{l})_N,$. The final node's hidden state is obtained by concatenation all heads' outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed046bcf",
   "metadata": {},
   "source": [
    "Here is the GAT layer that calculates nodes' preliminary hidden states by `self.fc` ($W_\\text{node}$) layer and copies them to the node feature `z`, then concatenates them on edges by `edge_cat` function and copies to the edge feature `z`, then calculates attention score by `self.attn_fc` ($W_\\text{edge}$) layer and performs message passing. At the end, it concatenates the next hidden states from all heads.\n",
    "\n",
    "_Remark: `graph.apply_edges` applies an edge-wise function on all edges in the graph._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df03d73",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "196cc47282c1195fda00e09bd227fe79",
     "grade": false,
     "grade_id": "cell-21314765531e0c74",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, n_heads):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, out_dim)\n",
    "        self.n_heads = n_heads\n",
    "        attn_fc = []\n",
    "        for _ in range(n_heads):\n",
    "            attn_fc.append(nn.Linear(2 * out_dim, 1))\n",
    "        self.attn_fc = nn.ModuleList(attn_fc)\n",
    "        \n",
    "    def forward(self, graph, feat):\n",
    "        with graph.local_scope():\n",
    "            graph = graph.add_self_loop()\n",
    "            graph.ndata['z'] = self.fc(feat)\n",
    "            graph.apply_edges(edge_cat)\n",
    "            edge_z = graph.edata['z']\n",
    "            h = []\n",
    "            for i in range(self.n_heads):\n",
    "                graph.edata['e'] = F.leaky_relu(self.attn_fc[i](edge_z))\n",
    "                graph.update_all(message_func, reduce_func)\n",
    "                h.append(graph.ndata['h'])\n",
    "            return torch.cat(h, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1864c2",
   "metadata": {},
   "source": [
    "In DGL, we can implement specific functions that can be applied on edges or nodes. Let us define an edge-wise function that concatenates nodes' preliminary hidden states `z` and copies them to the edge feature `z`. It takes a batch of edges `dgl.udf.EdgeBatch` which contains source node features in the attribute `src`, destination node features in the attribute `dst` and edge features in the attribute `data`. For example:\n",
    "* `edges.src['z']` is the source node feature `z`\n",
    "* `edges.dst['z']` is the destination node feature `z`\n",
    "* `edges.data['e']` is the edges feature `e`\n",
    "\n",
    "Write a function `edge_cat` that takes a batch of edges and returns a dictionary where the key is `z` and the value is concatenated the source node feature `z` and the destination node feature `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b35055e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "668cbae4fbc494c01a25c8b354f258ea",
     "grade": false,
     "grade_id": "cell-ce362a06bda2e6de",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def edge_cat(edges):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dba9efa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04a8035313508d82117b7cf138a93a3a",
     "grade": true,
     "grade_id": "cell-ac20c7b9a52d701a",
     "locked": true,
     "points": 0.67,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class EdgeBatch:\n",
    "    def __init__(self):\n",
    "        self.src = {'z': torch.arange(10).tile(3, 1)}\n",
    "        self.dst = {'z': torch.arange(10, 20).tile(3, 1)}\n",
    "        self.data = {'e': torch.arange(20).tile(5, 1)}\n",
    "edges = EdgeBatch()\n",
    "res = edge_cat(edges)\n",
    "assert res['z'].shape == (3, 20)\n",
    "assert torch.all(res['z'][0] == torch.arange(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554ac592",
   "metadata": {},
   "source": [
    "Next, implement the message function. Message function simply copies the source nodes' feature `z` and edges' feature `e` into mailboxes `z` and `e`.\n",
    "\n",
    "Write a function `message_func` that takes a batch of edges and returns a dictionary:\n",
    "* the key is `z`, the value is source nodes' feature `z`\n",
    "* the key is `e`, the value is edges' feature `e`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e0475d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8cc00e8a928114f9d63558d06f42a931",
     "grade": false,
     "grade_id": "cell-3a18badcbd54139b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def message_func(edges):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8c7b78",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c856676cd953bc1526a4e30294757ecd",
     "grade": true,
     "grade_id": "cell-2ef6cc6f7d9b708f",
     "locked": true,
     "points": 0.67,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class EdgeBatch:\n",
    "    def __init__(self):\n",
    "        self.src = {'z': torch.arange(10).tile(3, 1)}\n",
    "        self.dst = {'z': torch.arange(10, 20).tile(3, 1)}\n",
    "        self.data = {'e': torch.arange(20).tile(5, 1)}\n",
    "edges = EdgeBatch()\n",
    "res = message_func(edges)\n",
    "assert res['z'].shape == (3, 10)\n",
    "assert torch.all(res['z'][0] == torch.arange(10))\n",
    "assert res['e'].shape == (5, 20)\n",
    "assert torch.all(res['e'][0] == torch.arange(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f41c262",
   "metadata": {},
   "source": [
    "The reduce funtion takes a batch of nodes `dgl.udf.NodeBatch` and calculates the next hidden states of nodes by summation of preliminary hidden states `z` weighted by attention score:\n",
    "$$\\alpha_{ij} = \\text{Softmax}_i(e_{ij})$$\n",
    "$$h_i = \\sum_{j \\in \\mathcal{N}(i)}\\alpha_{ij}z_j$$\n",
    "\n",
    "The batch of nodes contains incoming messages in the attribute `mailbox` of the shape (N, D, 1), where N is the number of nodes in the batch, D is the number of messages received per node for this node batch. \n",
    "\n",
    "Write a function `reduce_func` that takes a batch of nodes with mailboxes `z` and `e` and returns a dictionary where the key is `h`, the value is the next hidden states of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dd2e63",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95573749111fe5a7223ef0070b2d0874",
     "grade": false,
     "grade_id": "cell-fa9346fd8971f05b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def reduce_func(nodes):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bb6114",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8bcfc66c7f372ac14df93ceefa350d26",
     "grade": true,
     "grade_id": "cell-6cf5795c738e58f8",
     "locked": true,
     "points": 0.67,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class NodeBatch:\n",
    "    def __init__(self):\n",
    "        self.mailbox = dict()\n",
    "        self.mailbox['e'] = torch.arange(10, 20, dtype=float).tile(3, 1)[..., None]\n",
    "        self.mailbox['z'] = torch.arange(10, dtype=float).tile(3, 1)[..., None]\n",
    "nodes = NodeBatch()\n",
    "assert reduce_func(nodes)['h'].shape == (3, 1)\n",
    "assert round(reduce_func(nodes)['h'][0].item(), 4) == 8.4185"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a14cf4",
   "metadata": {},
   "source": [
    "Write a class `GAT` that takes a graph and a feature matrix and performs two-layer convolution with intermediate ReLU activation. The class initialized by input, hidden, output dimensions and the number of heads in the first layer. The second layer contains a single head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ef1c65",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48273454d4e693bf7b20e8af8dde5196",
     "grade": false,
     "grade_id": "cell-57ec6b461b254ec6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, out_dim, n_heads):\n",
    "        super().__init__()\n",
    "        # self.conv1 = ...\n",
    "        # self.conv2 = ...\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    def forward(self, graph, feat):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23df8dcd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dea3fd3cc1a0f67d8cf0ec2b4d07cfe2",
     "grade": true,
     "grade_id": "cell-fbee928783f27b2a",
     "locked": true,
     "points": 0.67,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = GAT(2708, 32, 7, n_heads=4)\n",
    "logits = model.conv1(graph, torch.eye(2708))\n",
    "assert logits.shape == (2708, 128)\n",
    "logits = model(graph, torch.eye(2708))\n",
    "assert logits.shape == (2708, 7)\n",
    "assert logits.min() < 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c849aec",
   "metadata": {},
   "source": [
    "Let us initialize the model and the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fefc93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAT(in_dim=1433, hid_dim=16, out_dim=7, n_heads=3)\n",
    "opt = Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n",
    "log = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e76ba50",
   "metadata": {},
   "source": [
    "We train the model in semi-supervised setting: propagate information over the full graph, but optimize it by only train cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24f36b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 120\n",
    "for i in trange(n_epochs):\n",
    "    \n",
    "    logits = model(graph, feat)\n",
    "    train_loss = F.cross_entropy(logits[train_mask], label[train_mask])\n",
    "\n",
    "    opt.zero_grad()\n",
    "    train_loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loss = F.cross_entropy(logits[val_mask], label[val_mask])\n",
    "    \n",
    "    log.append([train_loss.item(), val_loss.item()])\n",
    "    \n",
    "plt.plot(np.array(log))\n",
    "plt.title('Loss')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66301f3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79efdeca8d9bd270d2ef0955d5fe5f4f",
     "grade": true,
     "grade_id": "cell-11d791f5122b19f3",
     "locked": true,
     "points": 0.67,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(graph, feat)\n",
    "y_pred = torch.argmax(logits[test_mask], 1)\n",
    "score = balanced_accuracy_score(label[test_mask], y_pred)\n",
    "assert score > 0.7\n",
    "print(f'Balanced accuracy: {score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c7e216",
   "metadata": {},
   "source": [
    "We can explain model's predictions using GAT — neighbors with large attention score have the great impact to model's prediction. Let us calculate the neighbors importance using the attention score.\n",
    "\n",
    "Write a function `neighbor_importance` that takes a node index for which we want to explain the prediction, a trained GAT, a graph and a feature matrix. It returns a tuple:\n",
    "* np.array with indices of neighbors\n",
    "* np.array with attention scores on the first GAT layer for each neighbor. Attention score is averaged among all heads.\n",
    "\n",
    "_Hint: use `graph.predecessors` to obtain neighbors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a81627",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bbc1d91f57b98dfcf437581e2b38a755",
     "grade": false,
     "grade_id": "cell-409230373547e910",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def neighbor_importance(expl_node, model, graph, feat):\n",
    "    with torch.no_grad(), graph.local_scope():\n",
    "        graph = graph.add_self_loop()\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6190ac",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e120b8bff1738540880015ee455dcb5f",
     "grade": true,
     "grade_id": "cell-9f74e551dbc38adf",
     "locked": true,
     "points": 0.6499999999999999,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "neigh, attn = neighbor_importance(1941, model, graph, feat)\n",
    "assert type(neigh) == type(attn) == np.ndarray\n",
    "assert neigh.shape == attn.shape == (7, )\n",
    "assert round(attn.sum(), 2) == 1\n",
    "assert 1397 == neigh[attn.argmax()] or 1937 == neigh[attn.argmax()]\n",
    "plt.bar(neigh.astype('str'), attn)\n",
    "plt.xlabel('Node')\n",
    "plt.ylabel('Attention')\n",
    "plt.title('Neighbor importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf826f6",
   "metadata": {},
   "source": [
    "### Task 5. Neighbor sampling with GraphSAGE (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308322f8",
   "metadata": {},
   "source": [
    "GraphSAGE (SAmple and AggreGatE) was proposed in [[7]](#references) as a model for inductive representation learning on large graph. There are two main things:\n",
    "* Neighbor sampling allows to train models in mini-batches, that is the model can be trained on large graphs and it also can represent previously unseen nodes (semi-supervised inductive learning)\n",
    "* Aggregation is perfomed separately on neighboring nodes and the node itself, that can be considered as \"skip-connections\" in graph neural networks\n",
    "\n",
    "Neighbor sampling is a technique to construct a message flow graph by randomly sampled neighbors of a target node.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/netspractice/network-science/main/images/neighbor_sampling.png' width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bddb502",
   "metadata": {},
   "source": [
    "In DGL, neighbor sampling is performed by `NodeDataLoader`. First, we create sets of indices with train, validation and test nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c056e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nids = graph.nodes()[train_mask]\n",
    "val_nids = graph.nodes()[val_mask]\n",
    "test_nids = graph.nodes()[test_mask]\n",
    "train_nids[:5], val_nids[:5], test_nids[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a59f3e7",
   "metadata": {},
   "source": [
    "Next, we create `NeighborSampler` with given number of neighbors for each layer of our network. Let it be 4 neihbors for the first layer and 4 neighbors for the second. Next, we create dataloader with the sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292d1347",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.NeighborSampler(fanouts=[4, 4])\n",
    "train_dataloader = dgl.dataloading.NodeDataLoader(graph, train_nids, sampler, batch_size=1)\n",
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ae454c",
   "metadata": {},
   "source": [
    "We have set batch size is 1, that is each batch contains a single target node. The number of train nodes is 140. Dataloader iterates over:\n",
    "* input nodes — the nodes from which we collect messages in the first layer\n",
    "* output nodes — the target nodes that aggregate messages from neighbors in the second layers\n",
    "* message flow graphs — list of graphs for layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6cba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_nodes, output_nodes, mfgs in train_dataloader:\n",
    "    break\n",
    "input_nodes, output_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04190b37",
   "metadata": {},
   "source": [
    "We see that the target node 0 aggregates messages from 0, 633, 1862, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49567254",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfgs[1].edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cdf063",
   "metadata": {},
   "source": [
    "The message flow graph (MFG) for the second layer consists of neighbors of the target nodes. Note that indices of nodes in MFG differs from indices in the original graph to optimize the message passing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95d9250",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfgs[1].srcnodes(), mfgs[1].dstnodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8edf40",
   "metadata": {},
   "source": [
    "MFG is bipartite, that is messages are passing from neighbors (source nodes) 0, 1, 2, 3 to the target (destination) node 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e130d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfgs[0].edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b74fcf",
   "metadata": {},
   "source": [
    "The first MFG consists of edges that follow from source nodes in the first layer (0, 1, 2, 3, 4, 5, 6, 7) to sourse nodes in the second layer (0, 1, 2, 3). Let us create train, validation and test dataloaders with batch size 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c703230",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = dgl.dataloading.NodeDataLoader(graph, train_nids, sampler, batch_size=64)\n",
    "val_dataloader = dgl.dataloading.NodeDataLoader(graph, val_nids, sampler, batch_size=64)\n",
    "test_dataloader = dgl.dataloading.NodeDataLoader(graph, test_nids, sampler, batch_size=64)\n",
    "len(train_dataloader), len(val_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f1688b",
   "metadata": {},
   "source": [
    "GraphSAGE is defined by message passing:\n",
    "\n",
    "$$h_i^l = W^l[h_i^{l-1} || \\text{Aggregation}_{j \\in \\mathcal{N}(i)}(h_j^{l-1})]$$\n",
    "\n",
    "where $[\\cdot||\\cdot]$ is concatenation. Aggregation function may be represented by average, maximum or LSTM. Let us define two-layer GraphSAGE with average aggregation and intermediate ReLU activation. `SAGEConv` layer takes the MFG and features of source node, returns the hidden of destinations nodes.\n",
    "\n",
    "Write a function `forward` that takes MFGs, feature matrix for input nodes and returns the next hidden states of output nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6d212a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8240c388d4a88c8baba1ad933da82ab",
     "grade": false,
     "grade_id": "cell-42631ac0523a62a7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_dim, hid_dim, aggregator_type='mean')\n",
    "        self.conv2 = SAGEConv(hid_dim, out_dim, aggregator_type='mean')\n",
    "\n",
    "    def forward(self, mfgs, feat):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e747b35e",
   "metadata": {},
   "source": [
    "We train the model in semi-supervised setting: propagate information over the full graph, but optimize it by only train cross-entropy loss.\n",
    "\n",
    "Write a function `train_epoch` that takes GraphSAGE model, train dataloader, validation dataloader, the full feature matrix, labels and optimizer. It returns a tuple: the average train loss and the average validation loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43c50da",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73d95920bbbccb1a805f27142fe43168",
     "grade": false,
     "grade_id": "cell-7a7eb641f7d14a3b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_dataloader, val_dataloader, feat, label, opt):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8776fb7b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "680b7450b92d8a2c0f5eaa77d0749eba",
     "grade": true,
     "grade_id": "cell-a6c94febdc58013b",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = GraphSAGE(1433, 16, 7)\n",
    "opt = Adam(model.parameters(), lr=0.1)\n",
    "train_loss, val_loss = train_epoch(model, train_dataloader, val_dataloader, feat, label, opt)\n",
    "assert type(train_loss) == float\n",
    "assert type(val_loss) == float\n",
    "assert 1.8 < train_loss < 2.2\n",
    "assert 1.8 < val_loss < 2.2\n",
    "train_loss, val_loss = train_epoch(model, train_dataloader, val_dataloader, feat, label, opt)\n",
    "train_loss, val_loss = train_epoch(model, train_dataloader, val_dataloader, feat, label, opt)\n",
    "assert train_loss < val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5530e48",
   "metadata": {},
   "source": [
    "Initialize the model and optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58dff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphSAGE(1433, 16, 7)\n",
    "opt = Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28def0b8",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55698c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = []\n",
    "n_epochs = 50\n",
    "for i in trange(n_epochs):\n",
    "    train_loss, val_loss = train_epoch(model, train_dataloader, val_dataloader, feat, label, opt)\n",
    "    log.append([train_loss, val_loss])\n",
    "plt.plot(np.array(log))\n",
    "plt.title('Loss')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0c16d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c5bce0504893fe5753e233815f38af4",
     "grade": true,
     "grade_id": "cell-1f31b207a4f89e8f",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "logits = []\n",
    "for input_nodes, output_nodes, mfgs in test_dataloader:\n",
    "    with torch.no_grad():\n",
    "        logits.append(model(mfgs, feat[input_nodes]))\n",
    "logits = torch.cat(logits)\n",
    "y_pred = torch.argmax(logits, 1)\n",
    "score = balanced_accuracy_score(label[test_mask], y_pred)\n",
    "assert score > 0.7\n",
    "print(f'Balanced accuracy: {score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41907817",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22487cd9",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "\n",
    "[1] Simonovsky, M., & Komodakis, N. (2018). GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders. CoRR, abs/1802.03480. http://arxiv.org/abs/1802.03480\n",
    "\n",
    "[2] Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., & Dahl, G. E. (2017). Neural Message Passing for Quantum Chemistry. CoRR, abs/1704.01212. http://arxiv.org/abs/1704.01212\n",
    "\n",
    "[3] Kipf, T. N., & Welling, M. (2016). Variational Graph Auto-Encoders. arXiv. https://doi.org/10.48550/ARXIV.1611.07308\n",
    "\n",
    "[4] Veličković, P., Cucurull, G., Casanova, A., Romero, A., Liò, P., & Bengio, Y. (2017). Graph Attention Networks. arXiv. https://doi.org/10.48550/ARXIV.1710.10903\n",
    "\n",
    "[5] Kipf, T. N., & Welling, M. (2016). Semi-Supervised Classification with Graph Convolutional Networks. CoRR, abs/1609.02907. http://arxiv.org/abs/1609.02907\n",
    "\n",
    "[6] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention Is All You Need. CoRR, abs/1706.03762. http://arxiv.org/abs/1706.03762\n",
    "\n",
    "[7] Hamilton, W. L., Ying, R., & Leskovec, J. (2017). Inductive Representation Learning on Large Graphs. CoRR, abs/1706.02216. http://arxiv.org/abs/1706.02216"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591768ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
